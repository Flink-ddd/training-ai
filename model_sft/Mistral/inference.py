import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_NAME = "./merged_mistral"
USE_MPS = torch.backends.mps.is_available()
# ğŸ”¹ è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆæ”¯æŒ MPSã€CUDA å’Œ CPUï¼‰
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

def chain_of_thought_prompt(question, context=""):
    """é€šç”¨ CoT æç¤ºè¯ï¼Œå¼•å¯¼æ¨¡å‹æŒ‰ç…§æ­¥éª¤æ¨ç†"""
    return f"""
Let's think step by step and solve this problem in a logical manner.

**Task:** {question}
{f"**Existing Context:** {context}" if context else ""}

### **Guidelines for Thought Process**
1. **Understanding the problem**:
   - Clearly define the problem statement.
   - Identify any relevant background knowledge or key concepts.
   - If applicable, break down any sub-problems.

2. **Breaking down the solution**:
   - List the necessary steps to solve the problem.
   - Consider any alternative approaches and compare their effectiveness.
   - Ensure logical consistency between steps.

3. **Executing the solution process**:
   - Apply each step systematically.
   - Use intermediate reasoning to validate correctness.
   - Provide relevant explanations or calculations if necessary.

4. **Providing the final answer**:
   - Summarize the key findings.
   - Ensure the answer directly addresses the original question.
   - If applicable, highlight any key takeaways or implications.

**Now, let's apply this structured reasoning to solve the given task.**
"""

def generate_response(model, tokenizer, question, history=None):
    """æ‰§è¡Œæ¨ç†å¹¶è¿”å›æ€ç»´é“¾æ ¼å¼çš„ç­”æ¡ˆ"""

    history = history or []  # é¿å… NoneType é—®é¢˜
    # ç»´æŒå¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    context = " ".join([f"User: {msg[0]} AI: {msg[1]}" for msg in history])


    prompt = chain_of_thought_prompt(question, context)

    tokenizer.pad_token = tokenizer.eos_token  # ç¡®ä¿ tokenizer ç»ˆæ­¢ç”Ÿæˆ

    # inputs = tokenizer(prompt, return_tensors="pt").to("mps" if USE_MPS else "cpu")
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(DEVICE)
    
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=512,  # å¢åŠ æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œç¡®ä¿å®Œæ•´æ¨ç†
            eos_token_id=tokenizer.eos_token_id,  # é‡åˆ°ç»ˆæ­¢ç¬¦æ—¶åœæ­¢
            repetition_penalty=1.2,  # è®©æ¨¡å‹å‡å°‘é‡å¤
            temperature=0.4,  # é™ä½æ¸©åº¦ï¼Œæé«˜é€»è¾‘æ€§  é™ä½éšæœºæ€§ï¼Œæé«˜é€»è¾‘æ€§
            top_p=0.9,  # é™åˆ¶é‡‡æ ·èŒƒå›´
            do_sample=True  # è®©å›ç­”æ›´æœ‰å˜åŒ–
        )
    
    return tokenizer.decode(output[0], skip_special_tokens=True)

if __name__ == "__main__":
    print(f"ğŸ”¹ åŠ è½½ {MODEL_NAME}ï¼Œä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")

    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE, device_map=DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    question = "Why do we feel sleepy at night?"
    response = generate_response(model, tokenizer, question)
    
    print("ğŸ’¡ ç»è¿‡ä¼˜åŒ–çš„å›ç­”ï¼š\n", response)