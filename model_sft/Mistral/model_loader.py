#ä½¿ç”¨Medusa å¯¹åŸæ¨¡å‹Mistral-7Bè¿›è¡Œæ¨ç†åŠ é€Ÿ
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
# from medusa import apply_medusa

MODEL_NAME = "./merged_mistral"
USE_MPS = torch.backends.mps.is_available()

def load_model():
    """åŠ è½½ LLM å¹¶ä¼˜åŒ–æ¨ç†"""
    device = "mps" if USE_MPS else "cpu"

    print(f"ğŸ”¹ åŠ è½½ {MODEL_NAME}ï¼Œä½¿ç”¨è®¾å¤‡ï¼š{device}")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, torch_dtype=torch.float16, device_map=device
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # model = apply_medusa(model)

    # ç›´æ¥ä½¿ç”¨é»˜è®¤çš„ Transformerï¼ˆä¸å†æ‰‹åŠ¨è½¬æ¢ BetterTransformerï¼‰
    # ä½ çš„ Mistral 7B ç‰ˆæœ¬ å·²ç»é»˜è®¤ä½¿ç”¨äº† BetterTransformer ä¼˜åŒ–ï¼Œæ‰€ä»¥ ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ BetterTransformer.transform(model)ã€‚
    model.eval()

    return model, tokenizer
