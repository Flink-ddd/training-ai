{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3f385",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stackoverflow.com-Posts.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(meta, f)  \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage:  \u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mparse_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstackoverflow.com-Posts.xml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstack_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 然后从 stack_data_meta.json 里重新加载 meta：\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mparse_xml\u001b[0;34m(xml_file, output_prefix)\u001b[0m\n\u001b[1;32m      7\u001b[0m meta \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;66;03m# Maps post IDs to metadata  \u001b[39;00m\n\u001b[1;32m      8\u001b[0m text_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_text.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m _, root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(context)  \u001b[38;5;66;03m# Skip root tag\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event, elem \u001b[38;5;129;01min\u001b[39;00m context:  \n",
      "File \u001b[0;32m/opt/anaconda3/envs/.conda/lib/python3.10/xml/etree/ElementTree.py:1272\u001b[0m, in \u001b[0;36miterparse\u001b[0;34m(source, events, parser)\u001b[0m\n\u001b[1;32m   1269\u001b[0m it\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m iterator, IterParseIterator\n\u001b[0;32m-> 1272\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m it\n",
      "File \u001b[0;32m/opt/anaconda3/envs/.conda/lib/python3.10/xml/etree/ElementTree.py:1249\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1249\u001b[0m         source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m         close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stackoverflow.com-Posts.xml'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET  \n",
    "from collections import defaultdict  \n",
    "import re  \n",
    "import json  \n",
    "\n",
    "def parse_xml(xml_file, output_prefix):  \n",
    "    meta = defaultdict(dict)  # Maps post IDs to metadata  \n",
    "    text_file = open(f\"{output_prefix}_text.tsv\", \"w\")\n",
    "    \n",
    "    context = ET.iterparse(xml_file, events=(\"start\",))  \n",
    "    _, root = next(context)  # Skip root tag\n",
    "    \n",
    "    for event, elem in context:  \n",
    "        if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"2\":  # Filter answers  \n",
    "            post_id = int(elem.attrib[\"Id\"])  \n",
    "            parent_id = int(elem.attrib.get(\"ParentId\", -1))  \n",
    "            score = int(elem.attrib.get(\"Score\", 0))  \n",
    "            body = elem.attrib[\"Body\"]\n",
    "            \n",
    "            # Store metadata  \n",
    "            meta[post_id] = {  \n",
    "                \"ParentId\": parent_id,  \n",
    "                \"Score\": score,  \n",
    "                \"Body\": body  \n",
    "            }  \n",
    "            text_file.write(f\"{post_id}\\t{body}\\n\")  \n",
    "            elem.clear()  # Free memory\n",
    "    \n",
    "    root.clear()  \n",
    "    text_file.close()  \n",
    "    with open(f\"{output_prefix}_meta.json\", \"w\") as f:  \n",
    "        json.dump(meta, f)  \n",
    "\n",
    "# Example usage:  \n",
    "parse_xml(\"Posts.xml\", \"stack_data\")\n",
    "# 然后从 stack_data_meta.json 里重新加载 meta：\n",
    "import json\n",
    "with open(\"stack_data_meta.json\") as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be31048",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([meta[aid][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m aid \u001b[38;5;129;01min\u001b[39;00m selected_aids])  \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, Y  \n\u001b[0;32m---> 25\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m create_balanced_labels(\u001b[43mmeta\u001b[49m)  \u001b[38;5;66;03m# meta from parse_xml  \u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mbincount(Y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict  \n",
    "import numpy as np  \n",
    "\n",
    "def create_balanced_labels(meta, num_questions=10000):  \n",
    "    question_answers = defaultdict(list)  \n",
    "    for aid, data in meta.items():  \n",
    "        if data[\"ParentId\"] != -1:  # Skip questions  \n",
    "            question_answers[data[\"ParentId\"]].append(aid)\n",
    "    \n",
    "    # Select top and bottom scoring answers per question  \n",
    "    selected_aids = []  \n",
    "    for qid, aids in list(question_answers.items())[:num_questions]:  \n",
    "        if len(aids) < 2:  \n",
    "            continue  \n",
    "        scores = [meta[aid][\"Score\"] for aid in aids]  \n",
    "        top_aid = aids[np.argmax(scores)]  \n",
    "        bottom_aid = aids[np.argmin(scores)]  \n",
    "        selected_aids.extend([top_aid, bottom_aid])\n",
    "    \n",
    "    # Create labels (Score > 0 as good)  \n",
    "    X = [meta[aid][\"Body\"] for aid in selected_aids]  \n",
    "    Y = np.array([meta[aid][\"Score\"] > 0 for aid in selected_aids])  \n",
    "    return X, Y  \n",
    "\n",
    "X, Y = create_balanced_labels(meta)  # meta from parse_xml  \n",
    "print(f\"Label distribution: {np.bincount(Y)}\")  # Should be ~50% each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5fb1ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/muxiaohui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  \n",
    "import nltk  \n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def extract_features(text):  \n",
    "    features = {}\n",
    "    \n",
    "    # HTML links (excluding those in code blocks)  \n",
    "    link_re = re.compile(r'<a href=\"http://.*?\">.*?</a>', re.IGNORECASE | re.DOTALL)  \n",
    "    code_re = re.compile(r'<pre>(.*?)</pre>', re.DOTALL)  \n",
    "    code_blocks = code_re.findall(text)  \n",
    "    text_no_code = code_re.sub(\"\", text)  \n",
    "    links = link_re.findall(text_no_code)  \n",
    "    features[\"link_count\"] = len(links)\n",
    "    \n",
    "    # Code lines  \n",
    "    code_lines = sum(len(block.split(\"\\n\")) for block in code_blocks)  \n",
    "    features[\"code_lines\"] = code_lines\n",
    "    \n",
    "    # Text complexity  \n",
    "    text_clean = re.sub(r'<.*?>', \"\", text_no_code).strip()  # Remove HTML tags  \n",
    "    tokens = word_tokenize(text_clean)  \n",
    "    features[\"word_count\"] = len(tokens)\n",
    "    \n",
    "    if tokens:  \n",
    "        sentences = sent_tokenize(text_clean)  \n",
    "        features[\"avg_sent_len\"] = np.mean([len(word_tokenize(s)) for s in sentences])  \n",
    "        features[\"avg_word_len\"] = np.mean([len(w) for w in tokens])\n",
    "    \n",
    "    # Stylistic features  \n",
    "    features[\"all_caps\"] = sum(1 for w in tokens if w.isupper())  \n",
    "    features[\"exclams\"] = text_clean.count(\"!\")\n",
    "    \n",
    "    return features  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90366c27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline  \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert dict features to matrix  \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m features \u001b[38;5;241m=\u001b[39m [extract_features(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX\u001b[49m] \n\u001b[1;32m      7\u001b[0m vec \u001b[38;5;241m=\u001b[39m DictVectorizer()  \n\u001b[1;32m      8\u001b[0m X_matrix \u001b[38;5;241m=\u001b[39m vec\u001b[38;5;241m.\u001b[39mfit_transform(features)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.pipeline import make_pipeline  \n",
    "\n",
    "# Convert dict features to matrix  \n",
    "features = [extract_features(text) for text in X] \n",
    "vec = DictVectorizer()  \n",
    "X_matrix = vec.fit_transform(features)  \n",
    "\n",
    "# Standardize features  \n",
    "scaler = StandardScaler()  \n",
    "X_standardized = scaler.fit_transform(X_matrix)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
